{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import json\n",
    "import pickle as pickle\n",
    "\n",
    "import trackintel as ti\n",
    "from trackintel.io.dataset_reader import read_geolife, geolife_add_modes_to_triplegs\n",
    "from trackintel.analysis.tracking_quality import temporal_tracking_quality, _split_overlaps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2.3\n"
     ]
    }
   ],
   "source": [
    "print(ti.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = os.path.join(\"..\", \"..\", \"paths.json\")\n",
    "with open(DATA_DIR) as json_file:\n",
    "    CONFIG = json.load(json_file)\n",
    "\n",
    "save_dir = os.path.join(\"..\", \"..\", CONFIG[\"data_dir\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 182/182 [01:33<00:00,  1.95it/s]\n"
     ]
    }
   ],
   "source": [
    "## read\n",
    "pfs, mode_labels = read_geolife(os.path.join(\"..\", \"..\", CONFIG[\"data_dir\"], \"Data\"), print_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([10, 20, 21, 52, 53, 56, 58, 59, 60, 62, 64, 65, 67, 68, 69, 73, 75, 76, 78, 80, 81, 82, 84, 85, 86, 87, 88, 89, 91, 92, 96, 97, 98, 100, 101, 102, 104, 105, 106, 107, 108, 110, 111, 112, 114, 115, 116, 117, 118, 124, 125, 126, 128, 129, 136, 138, 139, 141, 144, 147, 153, 154, 161, 163, 167, 170, 174, 175, 179])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mode_labels is a dictionary of pandas dataframes, with keys corresponding to the user id\n",
    "mode_labels.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identify users with high tracking quality (for tutorial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\envs\\reprotrack\\lib\\site-packages\\trackintel\\preprocessing\\positionfixes.py:113: UserWarning: 146900 duplicates were dropped from your positionfixes. Dropping duplicates is recommended but can be prevented using the 'exclude_duplicate_pfs' flag.\n",
      "  warnings.warn(warn_str)\n",
      "100%|██████████| 182/182 [00:28<00:00,  6.37it/s]\n",
      "c:\\ProgramData\\Anaconda3\\envs\\reprotrack\\lib\\site-packages\\trackintel\\preprocessing\\positionfixes.py:508: UserWarning: The positionfixes with ids [ 2894106  2894107  3826247  3826248  6900863  6900864  6909024  6909025\n",
      "  7754881  7754882 11128882 11128883 11608868 11608869 14216369 14216370\n",
      " 14411384 14411385 15674472 15674473 16107062 16107063 16129188 16129189\n",
      " 16477191 16477192 17290728 17290729 19072462 19072463] lead to invalid tripleg geometries. The resulting triplegs were omitted and the tripleg id of the positionfixes was set to nan\n",
      "  warnings.warn(warn_string)\n"
     ]
    }
   ],
   "source": [
    "# generate staypoints\n",
    "pfs, sp = pfs.generate_staypoints(gap_threshold=24 * 60, include_last=True, print_progress=True, dist_threshold=200, time_threshold=30, n_jobs=-1)\n",
    "\n",
    "# create activity flag\n",
    "sp = sp.create_activity_flag(method=\"time_threshold\", time_threshold=25)\n",
    "\n",
    "# generate triplegs\n",
    "pfs, tpls = pfs.generate_triplegs(sp, gap_threshold=15, print_progress=True)\n",
    "\n",
    "# generate trips\n",
    "sp, tpls, trips = sp.generate_trips(tpls, add_geometry=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting merge (28877, 9) (32970, 5)\n",
      "finished merge (69999, 13)\n",
      "**************************************************\n",
      "Total user number:  182\n"
     ]
    }
   ],
   "source": [
    "# prepare for calculating the tracking quality\n",
    "\n",
    "trips[\"started_at\"] = pd.to_datetime(trips[\"started_at\"]).dt.tz_localize(None)\n",
    "trips[\"finished_at\"] = pd.to_datetime(trips[\"finished_at\"]).dt.tz_localize(None)\n",
    "sp[\"started_at\"] = pd.to_datetime(sp[\"started_at\"]).dt.tz_localize(None)\n",
    "sp[\"finished_at\"] = pd.to_datetime(sp[\"finished_at\"]).dt.tz_localize(None)\n",
    "\n",
    "# merge trips and staypoints\n",
    "print(\"starting merge\", sp.shape, trips.shape)\n",
    "sp[\"type\"] = \"sp\"\n",
    "trips[\"type\"] = \"tpl\"\n",
    "df_all = pd.concat([sp, trips])\n",
    "df_all = _split_overlaps(df_all, granularity=\"day\")\n",
    "df_all[\"duration\"] = (df_all[\"finished_at\"] - df_all[\"started_at\"]).dt.total_seconds()\n",
    "print(\"finished merge\", df_all.shape)\n",
    "print(\"*\" * 50)\n",
    "\n",
    "print(\"Total user number: \", len(df_all[\"user_id\"].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get quality\n",
    "total_quality = temporal_tracking_quality(df_all, granularity=\"all\")\n",
    "# get tracking days\n",
    "total_quality[\"days\"] = (\n",
    "    df_all.groupby(\"user_id\").apply(lambda x: (x[\"finished_at\"].max() - x[\"started_at\"].min()).days).values\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter users tracked for more than 20 days and shorter than 200 days, and select the top 20 users with the highest tracking quality\n",
    "\n",
    "selected_users = total_quality[(total_quality[\"days\"] > 20) & (total_quality[\"days\"] < 200)].sort_values(by=\"quality\", ascending=False).head(20).sort_values(by=\"user_id\")[\"user_id\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1,   2,   7,   9,  11,  12,  14,  16,  19,  20,  22,  30,  35,\n",
       "        39,  41, 103, 112, 113, 154, 169], dtype=int64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select users from the original pfs and mode labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 182/182 [01:34<00:00,  1.92it/s]\n"
     ]
    }
   ],
   "source": [
    "pfs, mode_labels = read_geolife(os.path.join(\"..\", \"..\", CONFIG[\"data_dir\"], \"Data\"), print_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_pfs = pfs.loc[pfs[\"user_id\"].isin(selected_users)].drop(columns={\"elevation\", \"accuracy\"})\n",
    "\n",
    "selected_mode_labels = {}\n",
    "for key, value in mode_labels.items():\n",
    "    if key in selected_users:\n",
    "        selected_mode_labels[key] =  mode_labels[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4396670, dict_keys([20, 112, 154]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(selected_pfs), selected_mode_labels.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save to disc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save pfs\n",
    "selected_pfs.to_csv(os.path.join(save_dir, \"selected_geolife_pfs.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save mode labels\n",
    "with open(os.path.join(save_dir, \"selected_mode_labels.pk\"), \"wb\") as handle:\n",
    "    pickle.dump(selected_mode_labels, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check for tracking quality\n",
    "## Read files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_pfs = ti.read_positionfixes_csv(os.path.join(save_dir, \"selected_geolife_pfs.csv\"), index_col=\"id\")\n",
    "selected_mode_labels = pickle.load(open(os.path.join(save_dir, \"selected_mode_labels.pk\"), \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4396670, dict_keys([20, 112, 154]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# validate\n",
    "len(selected_pfs), selected_mode_labels.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate staypoints and trips "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\envs\\reprotrack\\lib\\site-packages\\trackintel\\preprocessing\\positionfixes.py:113: UserWarning: 1690 duplicates were dropped from your positionfixes. Dropping duplicates is recommended but can be prevented using the 'exclude_duplicate_pfs' flag.\n",
      "  warnings.warn(warn_str)\n",
      "100%|██████████| 20/20 [00:00<00:00, 36.13it/s]\n",
      "c:\\ProgramData\\Anaconda3\\envs\\reprotrack\\lib\\site-packages\\trackintel\\preprocessing\\positionfixes.py:508: UserWarning: The positionfixes with ids [2894106 2894107 3826247 3826248] lead to invalid tripleg geometries. The resulting triplegs were omitted and the tripleg id of the positionfixes was set to nan\n",
      "  warnings.warn(warn_string)\n"
     ]
    }
   ],
   "source": [
    "# generate staypoints, triplegs and trips\n",
    "pfs, sp = selected_pfs.generate_staypoints(gap_threshold=24 * 60, include_last=True, print_progress=True, dist_threshold=200, time_threshold=30, n_jobs=-1)\n",
    "\n",
    "# create activity flag\n",
    "sp = sp.create_activity_flag(method=\"time_threshold\", time_threshold=25)\n",
    "\n",
    "# generate triplegs\n",
    "pfs, tpls = pfs.generate_triplegs(sp, gap_threshold=15, print_progress=True)\n",
    "\n",
    "# generate trips\n",
    "sp, tpls, trips = sp.generate_trips(tpls, add_geometry=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate tracking quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting merge (5182, 8) (5531, 5)\n",
      "finished merge (11985, 12)\n",
      "**************************************************\n",
      "Total user number:  20\n"
     ]
    }
   ],
   "source": [
    "# prepare for calculating the tracking quality\n",
    "\n",
    "trips[\"started_at\"] = pd.to_datetime(trips[\"started_at\"]).dt.tz_localize(None)\n",
    "trips[\"finished_at\"] = pd.to_datetime(trips[\"finished_at\"]).dt.tz_localize(None)\n",
    "sp[\"started_at\"] = pd.to_datetime(sp[\"started_at\"]).dt.tz_localize(None)\n",
    "sp[\"finished_at\"] = pd.to_datetime(sp[\"finished_at\"]).dt.tz_localize(None)\n",
    "\n",
    "# merge trips and staypoints\n",
    "print(\"starting merge\", sp.shape, trips.shape)\n",
    "sp[\"type\"] = \"sp\"\n",
    "trips[\"type\"] = \"tpl\"\n",
    "df_all = pd.concat([sp, trips])\n",
    "df_all = _split_overlaps(df_all, granularity=\"day\")\n",
    "df_all[\"duration\"] = (df_all[\"finished_at\"] - df_all[\"started_at\"]).dt.total_seconds()\n",
    "print(\"finished merge\", df_all.shape)\n",
    "print(\"*\" * 50)\n",
    "\n",
    "print(\"Total user number: \", len(df_all[\"user_id\"].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.769783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.685497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>0.681411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>0.693301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>0.565434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>12</td>\n",
       "      <td>0.637580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>14</td>\n",
       "      <td>0.554814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>16</td>\n",
       "      <td>0.548101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>19</td>\n",
       "      <td>0.608273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>20</td>\n",
       "      <td>0.552151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>22</td>\n",
       "      <td>0.485112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>30</td>\n",
       "      <td>0.905935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>35</td>\n",
       "      <td>0.859254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>39</td>\n",
       "      <td>0.845132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>41</td>\n",
       "      <td>0.887421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>103</td>\n",
       "      <td>0.692618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>112</td>\n",
       "      <td>0.489056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>113</td>\n",
       "      <td>0.591597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>154</td>\n",
       "      <td>0.679012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>169</td>\n",
       "      <td>0.817202</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    user_id   quality\n",
       "0         1  0.769783\n",
       "1         2  0.685497\n",
       "2         7  0.681411\n",
       "3         9  0.693301\n",
       "4        11  0.565434\n",
       "5        12  0.637580\n",
       "6        14  0.554814\n",
       "7        16  0.548101\n",
       "8        19  0.608273\n",
       "9        20  0.552151\n",
       "10       22  0.485112\n",
       "11       30  0.905935\n",
       "12       35  0.859254\n",
       "13       39  0.845132\n",
       "14       41  0.887421\n",
       "15      103  0.692618\n",
       "16      112  0.489056\n",
       "17      113  0.591597\n",
       "18      154  0.679012\n",
       "19      169  0.817202"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get quality\n",
    "total_quality = temporal_tracking_quality(df_all, granularity=\"all\")\n",
    "total_quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### By week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get quality\n",
    "total_quality = temporal_tracking_quality(df_all, granularity=\"all\")\n",
    "total_quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### By day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yehong/mambaforge/envs/reprotrack/lib/python3.9/site-packages/trackintel/preprocessing/positionfixes.py:508: UserWarning: The positionfixes with ids [ 8088319  8088320 22941202 22941203  6660582  6660583 21005748 21005749\n",
      " 20967947 20967948   123109   123110   118896   118897   119237   119238\n",
      "   121376   121377   126464   126465 23445938 23445939  9935061  9935062] lead to invalid tripleg geometries. The resulting triplegs were omitted and the tripleg id of the positionfixes was set to nan\n",
      "  warnings.warn(warn_string)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### By weekday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### By hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check for mode split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pfs, tpls = pfs.generate_triplegs(sp, gap_threshold=15, print_progress=True)\n",
    "tpls = geolife_add_modes_to_triplegs(tpls, mode_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# assign mode\n",
    "tpls[\"pred_mode\"] = predict_transport_mode(tpls)[\"mode\"]\n",
    "tpls.loc[tpls[\"mode\"].isna(), \"mode\"] = tpls.loc[tpls[\"mode\"].isna(), \"pred_mode\"]\n",
    "tpls.drop(columns={\"pred_mode\"}, inplace=True)\n",
    "\n",
    "# get the length\n",
    "tpls[\"length_m\"] = calculate_haversine_length(tpls)\n",
    "\n",
    "groupsize = tpls.groupby(\"trip_id\").size().to_frame(name=\"triplegNum\").reset_index()\n",
    "tpls_group = tpls.merge(groupsize, on=\"trip_id\")\n",
    "\n",
    "# trips only with 1 triplegs\n",
    "res1 = tpls_group.loc[tpls_group[\"triplegNum\"] == 1][[\"trip_id\", \"length_m\", \"mode\"]].copy()\n",
    "\n",
    "# get the mode and length of remaining trips\n",
    "remain = tpls_group.loc[tpls_group[\"triplegNum\"] != 1].copy()\n",
    "remain.sort_values(by=\"length_m\", inplace=True, ascending=False)\n",
    "mode = remain.groupby(\"trip_id\").head(1).reset_index(drop=True)[[\"mode\", \"trip_id\"]]\n",
    "\n",
    "length = remain.groupby(\"trip_id\")[\"length_m\"].sum().reset_index()\n",
    "res2 = mode.merge(length, on=\"trip_id\")\n",
    "# concat the results\n",
    "res = pd.concat([res1, res2])\n",
    "res.rename(columns={\"trip_id\": \"id\"}, inplace=True)\n",
    "res.set_index(\"id\", inplace=True)\n",
    "\n",
    "trips_with_main_mode = trips.join(res, how=\"left\")\n",
    "trips_with_main_mode = trips_with_main_mode[~trips_with_main_mode[\"mode\"].isna()]\n",
    "trips_with_main_mode_cate = get_mode_geolife(trips_with_main_mode)\n",
    "\n",
    "print(trips_with_main_mode_cate[\"mode\"].value_counts())\n",
    "\n",
    "# filter activity staypoints\n",
    "sp = sp.loc[sp[\"is_activity\"] == True].drop(columns=[\"is_activity\", \"trip_id\", \"next_trip_id\"])\n",
    "\n",
    "# generate locations\n",
    "sp, locs = sp.as_staypoints.generate_locations(\n",
    "    epsilon=epsilon, num_samples=2, distance_metric=\"haversine\", agg_level=\"dataset\", n_jobs=-1, print_progress=True\n",
    ")\n",
    "# filter noise staypoints\n",
    "valid_sp = sp.loc[~sp[\"location_id\"].isna()].copy()\n",
    "\n",
    "# save locations\n",
    "locs = locs[~locs.index.duplicated(keep=\"first\")]\n",
    "filtered_locs = locs.loc[locs.index.isin(sp[\"location_id\"].unique())]\n",
    "\n",
    "path = Path(os.path.join(\".\", \"data\"))\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "filtered_locs.as_locations.to_csv(os.path.join(\".\", \"data\", f\"locations_{dataset}.csv\"))\n",
    "\n",
    "# merge staypoint with trips info\n",
    "sp = valid_sp.loc[~valid_sp[\"prev_trip_id\"].isna()].reset_index().copy()\n",
    "trips = (\n",
    "    trips_with_main_mode_cate.drop(columns=[\"started_at\", \"finished_at\", \"user_id\"])\n",
    "    .reset_index()\n",
    "    .rename(columns={\"id\": \"trip_id\"})\n",
    "    .copy()\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.18 ('reprotrack')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "vscode": {
   "interpreter": {
    "hash": "9597c3010ccf3b8705a75eca2c37e93b64c1720a9347287e0e23a990d1a82733"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
